{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section is based partly on Freedman, D.A., 2009. [_Statistical Models: Theory and Practice_, Revised Edition](http://www.amazon.com/Statistical-Models-Practice-David-Freedman/dp/0521743850/), Cambridge University Press.\n",
    "\n",
    "Statistical models, and regression in particular, are used primarily for three purposes:\n",
    "\n",
    "1. _Description_: to summarize data\n",
    "2. _Prediction_: to predict future data\n",
    "3. _Causal Inference_: to predict what would happen in response to an intervention\n",
    "\n",
    "It is straightforward to check whether a regression model is a good summary of _existing_ data, although there is some subtlety in determining whether the summary is _good enough_.  How to measure goodness of fit appropriately is not always obvious, and adequacy of fit depends on the use of the summary.\n",
    "\n",
    "Prediction is harder than description because it involves _extrapolation_: how can one tell what the future will bring? Why should the future be like the past? Is the system under study stable (i.e., _stationary_) or are its properties changing with time?\n",
    "\n",
    "However, the hardest of these tasks is causal inference. The biggest difficulty in drawing causal inferences is _confounding_, especially when the data arise from _observational studies_ rather than _randomized, controlled experiments_. (_Natural experiments_ lie somewhere in between; there are few that approach the utility of\n",
    "a randomized controlled experiment, but John Snow's study of the communication of cholera is a notable exception.)\n",
    "\n",
    "_Confounding_ happens when one factor or variable manifests in the outcome in a way that cannot be distinguished from the _treatment_.\n",
    "\n",
    "_Stratification_ (e.g., _cross tabulation_) can help reduce confounding. So can modeling&mdash;in some cases, but not in others. \n",
    "For modeling to help, it is generally necessary for the structure of the model to \n",
    "correspond to how the data were actually generated.\n",
    "Unfortunately, most models in science, especially social science, are chosen out of habit or computational\n",
    "convenience, not because they have a basis in the science itself.\n",
    "This often produces misleading results, and the misleading impression that those results have small\n",
    "uncertainties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some notation\n",
    "\n",
    "If $\\{x_i\\}_{i=1}^n$ is a list of numbers, \n",
    "$$\n",
    "\\bar{x} \\equiv \\frac{1}{n} \\sum_{i=1}^n\n",
    "$$\n",
    "is the _mean_ of the list;\n",
    "$$\n",
    "\\mbox{var} x \\equiv \\frac{1}{n} \\sum_{i=1}^n (x_i - \\bar{x})^2\n",
    "$$\n",
    "is the _variance_ of the list;\n",
    "$$\n",
    "s_x \\equiv \\sqrt{\\mbox{var} x}\n",
    "$$\n",
    "is the SD or _standard deviation_ of the list;\n",
    "and\n",
    "$$\n",
    "   z_i \\equiv \\frac{x_i - \\bar{x}}{s_x}\n",
    "$$\n",
    "is _$x_i$ in standard units_.\n",
    "With $\\bar{y}$ and $s_y$ defined analogously for the list $\\{y_i\\}_{i=1}^n$, and if $s_x$\n",
    "and $s_y$ are nonzero,\n",
    "\n",
    "$$\n",
    "   r_{xy} \\equiv \\frac{1}{n} \\sum_{i=1} \\frac{x_i - \\bar{x}}{s_x} \\cdot \\frac{y_i-\\bar{y}}{s_y}\n",
    "$$\n",
    "is the _correlation of $x$ and $y$_.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bivariate regression\n",
    "\n",
    "See [SticiGui: Regression](http://www.stat.berkeley.edu/~stark/SticiGui/Text/regression.htm)\n",
    "\n",
    "Suppose we observe pairs $\\{(x_i, y_i)\\}_{i=1}^n$.\n",
    "What straight line $y = a + bx$ comes closest to fitting these data, in the least-squares sense?\n",
    "That is, what values $a$ and $b$ minimize\n",
    "$$\n",
    "   \\mbox{mean squared residual} = \\mbox{MSR} = \\equiv \\frac{1}{n}\\sum_{i=1}^n \\left ( y_i - (a + bx_i) \\right )^2?\n",
    "$$\n",
    "The solution is $b = r_{xy} \\frac{s_y}{s_x}$ and $a = \\bar{y} - b \\bar{x}$.\n",
    "\n",
    "_Proof._\n",
    "The mean squared residual is \n",
    "$$\n",
    "  \\mbox{MSR} = \\frac{1}{n}\\sum_{i=1}^n \\left ( y_i - (a + bx_i) \\right )^2 = \n",
    "  \\frac{1}{n}\\sum_{i=1}^n \\left ( y_i^2 - 2y_i(a + bx_i) + (a+bx_i)^2 \\right ) =\n",
    "  \\frac{1}{n}\\sum_{i=1}^n \\left ( y_i^2 - 2y_i(a + bx_i) + a^2+2abx_i + b^2x_i^2 \\right ).\n",
    "$$\n",
    "\n",
    "This is quadratic in both $a$ and $b$, and has positive leading coefficients in both.\n",
    "Hence, its minimum occurs at a stationary point with respect to both $a$ and $b$.\n",
    "We can differentiate inside the sum and solve for the stationary point:\n",
    "\n",
    "$$\n",
    "   0 = \\frac{\\partial \\mbox{MSR}}{\\partial a} = \\frac{1}{n} \\sum_{i=1}^n 2(y_i - (a+b x_i))\n",
    "   = 2 (\\bar{y} - a - b \\bar{x}).\n",
    "$$\n",
    "\n",
    "Hence, the optimal $a$ solves $a = \\bar{y} - b \\bar{x}$.\n",
    "Substituting this into the expression for mean squared residual gives\n",
    "\n",
    "$$ \n",
    "\\mbox{MSR} = \\frac{1}{n} \\sum_{i=1}^n \\left ( y_i - (\\bar{y} - b \\bar{x} + bx_i) \\right )^2 \n",
    "= \\frac{1}{n} \\sum_{i=1}^n \\left ( y_i - (\\bar{y} - b (\\bar{x} - x_i)) \\right )^2.\n",
    "$$\n",
    "\n",
    "Differentiating with respect to $b$ and finding the stationary point gives\n",
    "$$\n",
    "   0 = \\frac{\\partial \\mbox{MSR}}{\\partial b} = \n",
    "   \\frac{1}{n} \\sum_{i=1}^n 2 \\left ( y_i - (\\bar{y} - b (\\bar{x} - x_i) ) \\right )(\\bar{x} - x_i)\n",
    "   = \\frac{1}{n} \\sum_{i=1}^n ( y_i - \\bar{y})(\\bar{x} - x_i) + \\frac{b}{n} \\sum_{i=1}^n (\\bar{x}-x_i)^2\n",
    "   = s_x s_y r_{xy} - b s_x^2,\n",
    "$$\n",
    "\n",
    "i.e., \n",
    "\n",
    "$$\n",
    "b = \\frac{s_x s_y r_{xy}}{s_x^2} = r_{xy}s_y/s_x.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some statistical terminology\n",
    "\n",
    "Suppose that $\\theta \\in \\Re^p$ is a parameter, and let $\\hat{\\theta}$ be an estimator of $\\theta$\n",
    "The _bias_ of $\\hat{\\theta}$ is\n",
    "\n",
    "$$\n",
    "  \\mbox{bias} \\hat{\\theta} = {\\mathbb E}(\\hat{\\theta} - \\theta) = {\\mathbb E} \\hat{\\theta} - \\theta.\n",
    "$$\n",
    "\n",
    "An estimate $\\hat{\\theta}$ of a parameter $\\theta \\in \\Theta$ is _unbiased_ \n",
    "if ${\\mathbb E}_\\eta \\hat{\\theta} = \\eta$ for all $\\eta \\in \\Theta$.\n",
    "\n",
    "The _mean squared error_ (MSE) of an estimate $\\hat{\\theta}$ of $\\theta$ is\n",
    "\n",
    "$$\n",
    "\\mbox{MSE } \\hat{\\theta} = {\\mathbb E} \\| \\hat{\\theta} - \\theta \\|^2.\n",
    "$$\n",
    "\n",
    "The _variance_ of $\\hat{\\theta}$ is\n",
    "\n",
    "$$\n",
    "\\mbox{var } \\hat{\\theta} = {\\mathbb E} \\| \\hat{\\theta} - {\\mathbb E} \\hat{\\theta} \\|^2.\n",
    "$$\n",
    "\n",
    "__Proposition.__\n",
    "\n",
    "$$\n",
    "\\mbox{MSE } \\hat{\\theta} = \\mbox{var } \\hat{\\theta} + \\| \\mbox{bias} \\hat{\\theta} \\|^2.\n",
    "$$\n",
    "\n",
    "_Proof._\n",
    "\n",
    "$$\n",
    "\\| \\hat{\\theta} - \\theta \\|^2 = \n",
    "\\| \\hat{\\theta} - {\\mathbb E} \\hat{\\theta} - (\\theta - {\\mathbb E} \\hat{\\theta}) \\|^2\n",
    "= (\\hat{\\theta} - {\\mathbb E} \\hat{\\theta} - (\\theta - {\\mathbb E} \\hat{\\theta}))'\n",
    "(\\hat{\\theta} - {\\mathbb E} \\hat{\\theta} - (\\theta - {\\mathbb E} \\hat{\\theta})) \n",
    "$$\n",
    "$$ =\n",
    "(\\hat{\\theta} - {\\mathbb E} \\hat{\\theta})'(\\hat{\\theta} - {\\mathbb E} \\hat{\\theta})\n",
    "+ 2 (\\hat{\\theta} - {\\mathbb E} \\hat{\\theta})'(\\theta - {\\mathbb E} \\hat{\\theta})\n",
    "+ (\\theta - {\\mathbb E} \\hat{\\theta})'(\\theta - {\\mathbb E} \\hat{\\theta})\n",
    "$$\n",
    "$$\n",
    "=\n",
    "\\| \\hat{\\theta} - {\\mathbb E} \\hat{\\theta} \\|^2 + \n",
    "2(\\hat{\\theta} - {\\mathbb E} \\hat{\\theta})'(\\theta - {\\mathbb E} \\hat{\\theta})\n",
    "+ \\| \\theta - {\\mathbb E} \\hat{\\theta} \\|^2.\n",
    "$$\n",
    "\n",
    "Hence,\n",
    "\n",
    "$$\n",
    "\\mbox{MSE } \\hat{\\theta} = {\\mathbb E} \\| \\hat{\\theta} - {\\mathbb E} \\hat{\\theta} \\|^2\n",
    "+ {\\mathbb E} \\left ( 2(\\hat{\\theta} - {\\mathbb E} \\hat{\\theta})'(\\theta - {\\mathbb E} \\hat{\\theta}) \\right )\n",
    "+ {\\mathbb E} \\| \\theta - {\\mathbb E} \\hat{\\theta} \\|^2\n",
    "$$\n",
    "$$\n",
    "=\n",
    "{\\mathbb E} \\| \\hat{\\theta} - {\\mathbb E} \\hat{\\theta} \\|^2\n",
    "+ 2 \\left ({\\mathbb E} \\hat{\\theta} - {\\mathbb E} \\hat{\\theta})'(\\theta - {\\mathbb E} \\hat{\\theta}) \\right )\n",
    "+ {\\mathbb E} \\| \\theta - {\\mathbb E} \\hat{\\theta} \\|^2\n",
    "$$\n",
    "$$\n",
    "= \\mbox{var } \\hat{\\theta} + \\| \\mbox{bias } \\hat{\\theta} \\|^2.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple  Regression\n",
    "\n",
    "Multiple linear regression is a commonly used tool in most branches of science, as well as economics.\n",
    "It is frequently misinterpreted.\n",
    "\n",
    "We will start with an introduction to the linear algebra of constructing the least-squares estimate for\n",
    "linear regression, then discuss the features and limitations of regression, especially the limitations\n",
    "of drawing causal inferences from regression models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notation\n",
    "\n",
    "The basic relationship for linear regression is the equation\n",
    "\n",
    "$$\n",
    "   Y = X \\beta + \\epsilon.\n",
    "$$\n",
    "\n",
    "Here, $Y$ is an $n$-vector of data, called the _dependent variable_, the _response_, the _explained variables_,\n",
    "the _modeled variables_, or the _left hand side$.\n",
    "The matrix $X \\in {\\mathcal M}(n,p)$ with $n \\ge p$, $\\mbox{rank}(X)=p$ (full rank)\n",
    "so that the columns of $X$ are linearly independent.\n",
    "\n",
    "The vector $\\beta$ is a $p$-vector of _parameters_, _model parameters_, or _coefficients_.  \n",
    "The usual goal of regression is to estimate $\\beta$.\n",
    "\n",
    "The vector $\\epsilon$ is an $n$-vector of _error_, _disturbance_, or _noise_.\n",
    "\n",
    "We will usually assume that $\\epsilon$ is random, which makes $Y$ random as well.\n",
    "We will usually treat $X$ as fixed rather than random.\n",
    "\n",
    "There is an observation $Y_i$ for each _unit_ of observation, a row of $X$ for each observation,\n",
    "and a column of $X$ for each parameter (element of $\\beta$).\n",
    "The columns correspond to _explanatory variables_, _independent variables_, _covariates_, _control variables_,\n",
    "or _right-hand side variables_.\n",
    "\n",
    "We have observaations of $Y$, which are _assumed_ to be values of $X\\beta + \\epsilon$.\n",
    "The value of $\\beta$ is unknown.\n",
    "The value of $\\epsilon$ cannot be observed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The standard assumption in multiple regression is that the noise terms $\\epsilon$ are random, with\n",
    "\n",
    "+ $\\{\\epsilon_i\\}$ iid (independent, identically distributed)\n",
    "+ ${\\mathbb E}\\epsilon_i = 0$\n",
    "+ $\\mbox{var}\\epsilon_i = \\sigma^2$, generally a known constant\n",
    "\n",
    "It is also standard to assume that if $X$ is random, $X$ and $\\epsilon$ are independent.\n",
    "Regardless, the value of $X$ is observed$.\n",
    "\n",
    "Since $X$ is observed, we can find $\\mbox{rank}(X)$.\n",
    "But there is no way to test whether the main assumptions are true, that is, whether\n",
    "+ $Y = X\\beta + \\epsilon$\n",
    "+ $\\{ \\epsilon_i \\}$ are iid with mean 0 and finite variance\n",
    "+ $X$ and $\\epsilon$ are independent.\n",
    "\n",
    "It is common to _condition_ on $X$; that is, to treat it as fixed erather than random."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ordinary least squares\n",
    "\n",
    "The ordinary least squares (OLS) estimate of $\\beta$ is\n",
    "\n",
    "$$ \n",
    "\\hat{\\beta} \\equiv \\left ( X'X  \\right )^{-1}X' Y.\n",
    "$$\n",
    "\n",
    "The estimate $\\hat{\\beta}$ is a $p$-vector, just like $\\beta$.\n",
    "\n",
    "The _residuals_ or _fitting errors_ are\n",
    "$$\n",
    "  e \\equiv Y - X \\hat{\\beta}.\n",
    "$$\n",
    "\n",
    "<hr />\n",
    "__Theorem.__\n",
    "\n",
    "1. $e \\perp X$ (i.e., $X'e = {\\mathbf 0}$: the fitting errors are orthogonal to $X$)\n",
    "1. $\\min_{\\gamma \\in \\Re^p} \\| Y - X\\gamma\\|^2 = \\| Y - X \\hat{\\beta}\\|^2$ ($\\hat{\\beta}$ solves the least squares fitting problem)\n",
    "<hr />\n",
    "\n",
    "This is a special case of the _Projection Theorem_.\n",
    "\n",
    "__Proof.__\n",
    "\n",
    "We prove the first part by direct calculation:\n",
    "$ X'e = X' (Y - X \\hat{\\beta})$, so\n",
    "\n",
    "$$ \\left ( X'X \\right )^{-1} X' e = \\left ( X'X \\right )^{-1} X' Y - \\left ( X'X \\right )^{-1} X'X \\hat{\\beta}\n",
    "= \\hat{\\beta} - \\hat{\\beta} = 0.\n",
    "$$\n",
    "\n",
    "For the second part, write $\\gamma = \\hat{\\beta} + (\\gamma - \\hat{\\beta})$.\n",
    "Then\n",
    "\n",
    "$$\n",
    "\\| Y - X \\gamma \\|^2 = \\| Y - X \\hat{\\beta} - X(\\gamma - \\beta) \\|^2 = \\| e - X (\\gamma - \\hat{\\beta})\\|^2\n",
    "$$\n",
    "$$\n",
    "= \\| e \\|^2 + 2 e' X(\\gamma - \\hat{\\beta}) + \\| X(\\gamma - \\hat{\\beta}) \\|^2\n",
    "\\ge \\| e \\|^2\n",
    "$$\n",
    "since $e'X = {\\mathbf 0}$.\n",
    "\n",
    "<hr />\n",
    "__Theorem.__\n",
    "The ordinary least squares estimate $\\hat{\\beta}$ is _conditionally unbiased_: ${\\mathbb E (\\hat{\\beta} \\mid X) = \\beta$.\n",
    "\n",
    "__Proof. __\n",
    "By calculation:\n",
    "\n",
    "$$ \n",
    "\\hat{\\beta} = (X'X)^{-1} X'Y = (X'X)^{-1} X'(X\\beta + \\epsilon)\n",
    "$$\n",
    "$$\n",
    "= (X'X)^{-1}X'X\\beta + (X'X)^{-1}X'\\epsilon\n",
    "= \\beta + (X'X)^{-1}X' \\epsilon.\n",
    "$$\n",
    "\n",
    "Now,\n",
    "$$ \n",
    "{\\mathbb E} \\left ( (X'X)^{-1} X' \\epsilon \\mid X \\right ) = (X'X)^{-1} X' E(\\epsilon \\mid X).\n",
    "$$\n",
    "\n",
    "Since $\\epsilon$ and $X$ are independent, \n",
    "$$\n",
    "   {\\mathbb E} (\\epsilon \\mid X) = {\\mathbb E}\\epsilon = 0.\n",
    "$$\n",
    "\n",
    "Hence,\n",
    "$$\n",
    "{\\mathbb E} (\\hat{\\beta} \\vert X) = {\\mathbb E}(\\beta + (X'X)^{-1}X'\\epsilon \\mid X) = \\beta + {\\mathbf 0} = \\beta.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computational example\n",
    "\n",
    "We drop an object from an unknown height $h$, with an unknown velocity $v$, subject to an unknown\n",
    "constant acceleration $a$.\n",
    "\n",
    "We measure the height $Y_i$ of the object at times $0, 1, 2, and 3$ seconds:\n",
    "\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "  <th> $t_i$ </th> <th>$Y_i$</th>\n",
    "</tr>\n",
    "<tr> <td> 0 </td> <td> 2.043 </td> </tr>\n",
    "<tr> <td> 1 </td> <td> 6.856 </td> </tr>\n",
    "<tr> <td> 2 </td> <td> 22.565 </td> </tr>\n",
    "<tr> <td> 3 </td> <td> 47.709 </td> </tr>\n",
    "</table>\n",
    "\n",
    "The height at time $t$ is\n",
    "\n",
    "$$\n",
    "  Y_i = h + v t_i + 1/2 a t_i^2 + \\epsilon_i.\n",
    "$$\n",
    "\n",
    "This has three unknown parameters: the initial height $h$, the initial velocity $v$, and the acceleration $a$.\n",
    "It is an (unverifiable)\n",
    "assumption that the noise terms $\\{\\epsilon_i\\}$ are independent and have zero mean and finite variance.\n",
    "\n",
    "We can expand the data relationship as follows:\n",
    "\n",
    "$$Y_1 = h + vt_1 + 1/2 a t_1^2 + \\epsilon_1 = h \\times 1 + v \\times 0 + a \\times 0 + \\epsilon_1$$\n",
    "$$Y_2 = h + vt_2 + 1/2 a t_2^2 + \\epsilon_2 = h \\times 1 + v \\times 1 + a \\times (1/2)1^2 + \\epsilon_2$$\n",
    "$$Y_3 = h + vt_3 + 1/2 a t_3^2 + \\epsilon_3 = h \\times 1 + v \\times 2 + a \\times (1/2)2^2 + \\epsilon_3$$\n",
    "$$Y_4 = h + vt_4 + 1/2 a t_3^4 + \\epsilon_4 = h \\times 1 + v \\times 3 + a \\times (1/2)3^2 + \\epsilon_3.$$\n",
    "\n",
    "We can write these relations in matrix form, as follows.\n",
    "\n",
    "Define\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "   2.043 \\\\\n",
    "   6.856 \\\\\n",
    "   22.565 \\\\\n",
    "   47.709\n",
    "\\end{pmatrix},\n",
    "$$\n",
    "\n",
    "$$\n",
    "Y \\equiv\n",
    "$$\n",
    " X \\equiv \\begin{pmatrix}\n",
    " 1 & 0 & 0 \\\\\n",
    " 1 & 1 & 0.5 \\\\\n",
    " 1 & 2 & 2 \\\\\n",
    " 1 & 3 & 4.5\n",
    "\\end{pmatrix},\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\beta \\equiv \\begin{pmatrix}\n",
    "h \\\\\n",
    "v \\\\\n",
    "a\n",
    "\\end{pmatrix},\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "\n",
    "\\epsilon = \\begin{pmatrix}\n",
    "\\epsilon_1 \\\\\n",
    "\\epsilon_2 \\\\\n",
    "\\epsilon_3 \\\\\n",
    "\\epsilon_4 \\\\\n",
    "\\end{pmatrix}\n",
    ".\n",
    "$$\n",
    "\n",
    "Then the data relationship can be written\n",
    "\n",
    "$$\n",
    "Y = X \\beta + \\epsilon.\n",
    "$$\n",
    "\n",
    "The least squares estimate $\\hat{beta}$ of $\\beta$ is\n",
    "$$\n",
    "\\hat{\\beta} = (X'X)^{-1}X'Y.\n",
    "$$\n",
    "If the Newtonian gravity data relations are correct and the observational\n",
    "noise $\\epsilon$ really has zero mean, then _formally_ $\\hat{\\beta}$ is an unbiased\n",
    "estimate of $\\beta$.\n",
    "However, we should not calculate $\\hat{\\beta}$ by inverting $X'X$, as discussed previously.\n",
    "Rather, we should use matrix factorization or back-substitution.\n",
    "\n",
    "In R, we would find $\\hat{\\beta}$ as follows.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "\t<tr><td>1</td><td>0</td><td>0</td></tr>\n",
       "\t<tr><td>1.0</td><td>1.0</td><td>0.5</td></tr>\n",
       "\t<tr><td>1</td><td>2</td><td>2</td></tr>\n",
       "\t<tr><td>1.0</td><td>3.0</td><td>4.5</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{lll}\n",
       "\t 1 & 0 & 0\\\\\n",
       "\t 1.0 & 1.0 & 0.5\\\\\n",
       "\t 1 & 2 & 2\\\\\n",
       "\t 1.0 & 3.0 & 4.5\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "1. 1\n",
       "2. 1\n",
       "3. 1\n",
       "4. 1\n",
       "5. 0\n",
       "6. 1\n",
       "7. 2\n",
       "8. 3\n",
       "9. 0\n",
       "10. 0.5\n",
       "11. 2\n",
       "12. 4.5\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "     [,1] [,2] [,3]\n",
       "[1,]    1    0  0.0\n",
       "[2,]    1    1  0.5\n",
       "[3,]    1    2  2.0\n",
       "[4,]    1    3  4.5"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "\t<tr><td>1.96995</td></tr>\n",
       "\t<tr><td>0.02245</td></tr>\n",
       "\t<tr><td>10.1655</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{l}\n",
       "\t 1.96995\\\\\n",
       "\t 0.02245\\\\\n",
       "\t 10.1655\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "1. 1.96995\n",
       "2. 0.0224499999999926\n",
       "3. 10.1655\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "         [,1]\n",
       "[1,]  1.96995\n",
       "[2,]  0.02245\n",
       "[3,] 10.16550"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Least squares example in R\n",
    "Y <- c(2.043, 6.856, 22.565, 47.709);\n",
    "X <- matrix(c(1,0,0,1,1,0.5,1,2,2,1,3,9/2), nrow = 4, ncol=3, byrow = T);\n",
    "X\n",
    "betahat <- solve(t(X) %*% X, t(X) %*% Y);\n",
    "betahat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is, we would estimate that the object was dropped from a height of 1.970 m\n",
    "with an initial velocity of 0.022 m/s, under gravitational acceleration of 10.166 meters/s<sup>2.\n",
    "\n",
    "Let's find the residual vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "\t<tr><td>0.07305</td></tr>\n",
       "\t<tr><td>-0.21915</td></tr>\n",
       "\t<tr><td>0.21915</td></tr>\n",
       "\t<tr><td>-0.07305</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{l}\n",
       "\t 0.07305\\\\\n",
       "\t -0.21915\\\\\n",
       "\t 0.21915\\\\\n",
       "\t -0.07305\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "1. 0.0730499999999983\n",
       "2. -0.219149999999998\n",
       "3. 0.219149999999999\n",
       "4. -0.0730500000000092\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "         [,1]\n",
       "[1,]  0.07305\n",
       "[2,] -0.21915\n",
       "[3,]  0.21915\n",
       "[4,] -0.07305"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e <- Y - X %*% betahat\n",
    "e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.1.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
