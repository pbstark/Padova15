{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probability, Random Vectors, and the Multivariate Normal Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability\n",
    "\n",
    "This course assumes that you have already had an introduction to probability, random variables, expectation, and so on, so the treatment is very cursory.\n",
    "The treatment is quite informal, omitting important technical concepts such as sigma-algebras,\n",
    "measurability, and the like.\n",
    "\n",
    "Modern probability theory starts with Kolmogorov's Axioms; here is an informal startement of the axioms.\n",
    "For more (but still a very informal treatment), see these chapters of SticiGui: \n",
    "[Probability: Philosophy and Mathematical Background](http://www.stat.berkeley.edu/~stark/SticiGui/Text/probabilityPhilosophy.htm),\n",
    "[Set theory](http://www.stat.berkeley.edu/~stark/SticiGui/Text/sets.htm),\n",
    "and\n",
    "[Probability: Axioms and Fundaments](http://www.stat.berkeley.edu/~stark/SticiGui/Text/probabilityAxioms.htm).\n",
    "\n",
    "Let $S$ denote the _outcome space_, the set of all possible outcomes of a random experiment, \n",
    "and let $\\{A_i\\}_{i=1}^\\infty$ be subsets of $S$.\n",
    "(Note that here $A$ denotes a subset, not a matrix.)\n",
    "Then any probability function ${\\mathbb P}$ must satisfy these axioms:\n",
    "\n",
    "1. For every $A \\subset S$, ${\\mathbb P}(A) \\ge 0$ (probabilities are nonnegative)\n",
    "2. ${\\mathbb P}(S) = 1$ (the chance that _something_ happens is 100%)\n",
    "3. If $A_i \\cap A_j = \\emptyset$ for $i \\ne j$, then ${\\mathbb P} \\cup_{i=1}^\\infty A_i = \\sum_{i=1}^\\infty {\\mathbb P}(A_i)$\n",
    "(If a countable collection of events is _pairwise disjoint_, then the chance that any of the\n",
    "events occurs is the sum of the chances that they occur individually.)\n",
    "\n",
    "These axioms have many useful consequences, among them that ${\\mathbb P}(\\emptyset) = 0$, ${\\mathbb P}(A^c) = 1 - {\\mathbb P}(A)$,\n",
    "and ${\\mathbb P}(A \\cup B) = {\\mathbb P}(A) + {\\mathbb P}(B) - {\\mathbb P}(AB)$.\n",
    "\n",
    "<hr />\n",
    "### Definitions\n",
    "\n",
    "Let $A$ and $B$ be subsets of outcome space $S$.\n",
    "\n",
    "+ If $AB = \\emptyset$, then $A$ and $B$ are _mutually exclusive_.\n",
    "+ If ${\\mathbb P}(AB) = {\\mathbb P}(A){\\mathbb P}(B)$, then $A$ and $B$ are _independent_.\n",
    "+ If ${\\mathbb P}(B) > 0$, then the _conditional probability of $A$ given $B$_ is\n",
    "${\\mathbb P}(A | B) \\equiv {\\mathbb P}(AB)/{\\mathbb P}(B)$.\n",
    "<hr />\n",
    "\n",
    "Independence is an extremely specific relationship.\n",
    "At one extreme, $AB = \\emptyset$; then ${\\mathbb P}(AB) = 0 \\le {\\mathbb P}(A){\\mathbb P}(B)$. \n",
    "At another extreme, either $A$ is a subset of $B$ or vice versa; then ${\\mathbb P}(AB) = \\min({\\mathbb P}(A),{\\mathbb P}(B)) \\ge {\\mathbb P}(A){\\mathbb P}(B)$.\n",
    "Independence lies at a precise point in between."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random variables\n",
    "\n",
    "Briefly, a _real-valued random variable_ $X$ can be characterized by its probability distribution, which specifies (for a suitable collection of subsets of the real line $\\Re$ that comprises a sigma-algebra), the chance that the value of $X$ will be in each such subset.\n",
    "There are technical requirements regarding  _measurability_, which generally we will ignore.\n",
    "Perhaps the most natural mathematical setting for probability theory involves _Lebesgue integration_;\n",
    "we will largely ignore the difference between a _Riemann integral_ and a _Lebesgue integral_.\n",
    "\n",
    "Let $P_X$ denote the probability distribution of the random variable $X$. \n",
    "Then if $A \\subset \\Re$, $P_X(S) = {\\mathbb P} \\{ X \\in A \\}$.\n",
    "Ee write $X \\sim P_X$,\n",
    "pronounced \"$X$ is distributed as $P_X$\" or \"$X$ has distribution $P_X$.\" \n",
    "\n",
    "If two random variables $X$ and $Y$ have the same distribution, we write $X \\sim Y$ and we say that $X$ and $Y$\n",
    "are _identically distributed_.\n",
    "\n",
    "Real-valued random variables can be _continuous_, _discrete_, or _mixed (general)_.\n",
    "\n",
    "Continuous random variables have _probability density functions_ with respect to Lebesgue measure.\n",
    "If $X$ is a continuous random variables, there is some nonnegative function $f(x)$,\n",
    "the probability density of $X$, such that\n",
    "for any (suitable) set $A \\subset \\Re$,\n",
    "$$\n",
    "  {\\mathbb P} \\{ X \\in A \\} = \\int_A f(x) dx.\n",
    "$$\n",
    "Since ${\\mathbb P} \\{ X \\in \\Re \\} = 1$, it follows that $\\int_{-\\infty}^\\infty f(x) dx = 1$.\n",
    "\n",
    "_Example._ \n",
    "Let $f(x) = \\lambda e^{-\\lambda x}$ for $x \\ge 0$, and $f(x) = 0$ otherwise.\n",
    "Clearly $f(x) \\ge 0$.\n",
    "$$\n",
    "  \\int_{-\\infty}^\\infty f(x) dx = \\int_0^\\infty \\lambda e^{-\\lambda x} dx\n",
    "  = - e^{-\\lambda x}|_0^\\infty = - 0 + 1 = 1.\n",
    "$$\n",
    "Hence, $\\lambda e^{-\\lambda x}$ can be the probability density of a continuous random variable.\n",
    "A random variable with this density is said to be _exponentially distributed_.\n",
    "Exponentially distributed random variables are used to model radioactive decay and the failure\n",
    "of items that do not \"fatigue.\" For instance, the lifetime of a semiconductor after an initial\n",
    "\"burn-in\" period is often modeled as an exponentially distributed random variable.\n",
    "It is also a common model for the occurrence of earthquakes (although it does not fit the data well).\n",
    "\n",
    "_Example._\n",
    "Let $a$ and $b$ be real numbers with $a < b$, and let $f(x) = \\frac{1}{b-a}$, $x \\in [a, b]$ and \n",
    "$f(x)=0$, otherwise. \n",
    "Then $f(x) \\ge 0$ and $\\int_{-\\infty}^\\infty f(x) dx = \\int_a^b \\frac{1}{b-a} = 1$,\n",
    "so $f(x)$ can be the probability density function of a continuous random variable.\n",
    "A random variable with this density is sad to be _uniformly distributed on the interval $[a, b]$_.\n",
    "\n",
    "Discrete random variables assign all their probability to some _countable_ set of points $\\{x_i\\}_{i=1}^n$,\n",
    "where $n$ might be infinite.\n",
    "Discrete random variables have _probability mass functions_.\n",
    "If $X$ is a discrete random variable, there is a nonnegative function $p$, the probability mass function\n",
    "of $X$, such that\n",
    "for any set $A \\subset \\Re$,\n",
    "$$\n",
    "  {\\mathbb P} \\{X \\in A \\} = \\sum_{i: x_i \\in A} p(x_i).\n",
    "$$\n",
    "The value $p(x_i) = {\\mathbb P} \\{X = x_i\\}$, and $\\sum_{i=1}^\\infty p(x_i) = 1$.\n",
    "\n",
    "_Example._\n",
    "Let $x_i = i-1$ for $i=1, 2, \\ldots$, and let $p(x_i) = e^{-\\lambda} \\lambda^{x_i}/x_i!$.\n",
    "Then $p(x_i) > 0$ and \n",
    "$$ \n",
    "\\sum_{i=1}^\\infty p(x_i) = e^{-\\lambda} \\sum_{j=0}^\\infty \\lambda^j/j! = e^{-\\lambda} e^{\\lambda} = 1.\n",
    "$$\n",
    "Hence, $p(x)$ is the probability mass function of a discrete random variable.\n",
    "A random variable with this probability mass function is said to be _Poisson distributed (with parameter\n",
    "$\\lambda$)_.\n",
    "Poisson-distributed random variables are often used to model rare events.\n",
    "\n",
    "\n",
    "_Example._\n",
    "Let $x_i = i$ for $i=1, \\ldots, n$, and let $p(x_i) = 1/n$ and $p(x) = 0$, otherwise.\n",
    "Then $p(x) \\ge 0$ and $\\sum_{x_i} p(x_i) = 1$.\n",
    "Hence, $p(x)$ can be the probability mass function of a discrete random variable.\n",
    "A random variable with this probability mass function is said to be _uniformly distributed on $1, \\ldots, n$_.\n",
    "\n",
    "_Example._\n",
    "Let $x_i = i-1$ for $i=1, \\ldots, n+1$, and let $p(x_i) = {n \\choose x_i} p^{x_i} (1-p)^{n-x_i}$, and\n",
    "$p(x) = 0$ otherwise.\n",
    "Then $p(x) \\ge 0$ and \n",
    "$$\n",
    "\\sum_{x_i} p(x_i) = \\sum_{j=0}^n {n \\choose j} p^j (1-p)^{n-j} = 1,\n",
    "$$\n",
    "by the binomial theorem.\n",
    "Hence $p(x)$ is the probability mass function of a discrete random variable.\n",
    "A random variable with this probability mass function is said to be _binomially distributed\n",
    "with parameters $n$ and $p$_.\n",
    "The number of successes in $n$ independent trials that each have the same probability $p$ of success\n",
    "has a binomial distribution with parameters $n$ and $p$\n",
    "For instance, the number of times a fair die lands with 3 spots showing in 10 independent rolls has\n",
    "a binomial distribution with parameters $n=10$ and $p = 1/6$.\n",
    "\n",
    "For general random variables, the chance that $X$ is in some subset of $\\Re$ cannot be written as\n",
    "a sum or as a Riemann integral; it is more naturally represented as a Lebesgue integral (with respect to\n",
    "a measure other than Lebesgue measure).\n",
    "For example, imagine a random variable $X$ that has probability $\\alpha$ of being equal to zero;\n",
    "and if $X$ is not zero, it has a uniform distribution on the interval $[0, 1]$.\n",
    "Such a random variable is neither continuous nor discrete.\n",
    "\n",
    "Most of the random variables in this class are either discrete or continuous.\n",
    "\n",
    "If $X$ is a random variable such that, for some constant $x_1 \\in \\Re$, ${\\mathbb P}(X = x_1) = 1$, $X$\n",
    "is called a _constant random variable_.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "+ Show analytically that $\\sum_{x_i} p(x_i) = \\sum_{j=0}^n {n \\choose j} p^j (1-p)^{n-j} = 1$.\n",
    "+ Write an R script that verifies that equation numerically for $n=10$: for 1000 values of $p$ \n",
    "equispaced on the interval $(0, 1)$, find the maximum absolute value of the difference between the sum and\n",
    "1.\n",
    "+ Let $ \\in (0, 1]$; let $x_i = 1, 2, \\ldots$; and define $p(x_i) = (1-p)^{x_i-1}p$, and $p(x) = 0$ otherwise. Show analytically that $p(x)$ is the probability mass function of a discrete random variable. \n",
    "(A random variable with this probability mass function is said to be _geometricallly distributed\n",
    "with parameter $p$_.)\n",
    "+ Starting from Kolmogorov's three axioms, show that ${\\mathbb P}(A \\cup B) = {\\mathbb P}(A) + {\\mathbb P}(B) - {\\mathbb P}(AB)$.\n",
    "+ Starting from Kolmogorov's three axioms, show that \n",
    "${\\mathbb P}(A \\cup B \\cup C) = {\\mathbb P}(A) + {\\mathbb P}(B) + {\\mathbb P}(C) - {\\mathbb P}(AB) - {\\mathbb P}(AC) - {\\mathbb P}(BC) + {\\mathbb P}(ABC)$.\n",
    "+ Starting from Kolmogorov's three axioms, show that ${\\mathbb P}(AB) \\le \\min({\\mathbb P}(A),{\\mathbb P}(B))$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jointly Distributed Random Variables\n",
    "\n",
    "Often we work with more than one random variable at a time.\n",
    "Indeed, much of this course concerns _random vectors_, the components of which are individual\n",
    "real-valued random variables.\n",
    "\n",
    "The _joint probability distribution_ of a collection of random variables $\\{X_i\\}_{i=1}^n$ gives the probability that\n",
    "the variables simultaneously fall in subsets of their possible values.\n",
    "That is, for every (suitable) subset $ A \\in \\Re^n$, the joint probability distribution of $\\{X_i\\}_{i=1}^n$\n",
    "gives ${\\mathbb P} \\{ (X_1, \\ldots, X_n) \\in A \\}$.\n",
    "\n",
    "An _event determined by the random variable $X$_ is an event of the form $X \\in A$, where $A \\subset \\Re$.\n",
    "\n",
    "An _event determined by the random variables $\\{X_j\\}_{j \\in J}$_ is an event of the form\n",
    "$(X_j)_{j \\in J} \\in A$, where $A \\subset \\Re^{\\#J}$.\n",
    "\n",
    "Two random variables $X_1$ and $X_2$ are _independent_ if every event determined by $X_1$ is independent\n",
    "of every event determined by $X_2$.\n",
    "If two random variables are not independent, they are _dependent_.\n",
    "\n",
    "A collection of random variables $\\{X_i\\}_{i=1}^n$ is _independent_ if every event determined by every subset\n",
    "of those variables is independent of every event determined by any disjoint subset of those variables.\n",
    "If a collection of random variables is not independent, it is _dependent_.\n",
    "\n",
    "Loosely speaking, a collection of random variables is independent if learning the values of some of them\n",
    "tells you nothing about the values of the rest of them.\n",
    "If learning the values of some of them tells you anything about the values of the rest of them,\n",
    "the collection is dependent.\n",
    "\n",
    "For instance, imagine tossing a fair coin twice and rolling a fair die.\n",
    "Let $X_1$ be the number of times the coin lands heads, and $X_2$ be the number of spots that show on the die.\n",
    "Then $X_1$ and $X_2$ are independent: learning how many times the coin lands heads tells you nothing about what\n",
    "the die did.\n",
    "\n",
    "On the other hand, let $X_1$ be the number of times the coin lands heads, and let $X_2$ be the sum of the\n",
    "number of heads and the number of spots that show on the die.\n",
    "Then $X_1$ and $X_2$ are dependent. For instance, if you know the coin landed heads twice, you know that the sum\n",
    "of the number of heads and the number of spots must be at least 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expectation\n",
    "\n",
    "See [SticiGui: The Long Run and the Expected Value](http://www.stat.berkeley.edu/~stark/SticiGui/Text/expectation.htm) for an elementary introduction to expectation.\n",
    "\n",
    "The _expectation_ or _expected value_ of a random variable $X$, denoted ${\\mathbb E}X$, is a probability-weighted average of its possible values.\n",
    "From a frequentist perspective, it is the long-run limit (in probabiity) of the average of its values in repeated experiments.\n",
    "The expected value of a real-valued random variable (when it exists) is a fixed number, not a random value.\n",
    "The expected value depends on the probability distribution of $X$ but not on any realized value of $X$.\n",
    "If two random variables have the same probability distribution, they have the same expected value.\n",
    "\n",
    "<hr />\n",
    "### Properties of Expectation\n",
    "\n",
    "+ For any real $\\alpha \\in \\Re$, if ${\\mathbb P} \\{X = \\alpha\\} = 1$, then ${\\mathbb E}X = \\alpha$: the expected\n",
    "value of a constant random variable is that constant.\n",
    "+ For any real $\\alpha \\in \\Re$, ${\\mathbb E}(\\alpha X) = \\alpha {\\mathbb E}X$: scalar homogeneity.\n",
    "+ If $X$ and $Y$ are random variables, ${\\mathbb E}(X+Y) = {\\mathbb E}X + {\\mathbb E}Y$: additivity.\n",
    "\n",
    "<hr />\n",
    "\n",
    "### Calculating Expectation\n",
    "If $X$ is a continuous real-valued random variable with density $f(x)$, then the expected value of $X$ is\n",
    "$$\n",
    "   {\\mathbb E}X = \\int_{-\\infty}^\\infty x f(x) dx,\n",
    "$$\n",
    "provided the integral exists.\n",
    "\n",
    "_Example._\n",
    "Suppose $X$ has density $f(x) = \\frac{1}{b-a}$ for $a \\le x \\le b$ and $0$ otherwise.\n",
    "Then $ {\\mathbb E}X = \\int_{-\\infty}^\\infty x f(x) dx = \\frac{1}{b-a} \\int_a^b x dx = \\frac{b^2-a^2}{2(b-a)} =\n",
    "\\frac{a+b}{2}$.\n",
    "\n",
    "If $X$ is a discrete real-valued random variable with probability function $p$, then the expected value of $X$ is\n",
    "$$\n",
    "   {\\mathbb E}X = \\sum_{i=1}^\\infty x_i p(x_i),\n",
    "$$\n",
    "where $\\{x_i\\} = \\{ x \\in \\Re: p(x) > 0\\}$,\n",
    "provided the sum exists.\n",
    "\n",
    "_Example._\n",
    "Suppose $X$ has a Poisson distribution with parameter $\\lambda$.\n",
    "Then ${\\mathbb E}X = e^{-\\lambda} \\sum_{j=0}^\\infty j \\lambda^j/j! = \\lambda$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variance,  Standard Error, and Covariance\n",
    "\n",
    "The _variance_ of a random variable $X$ is $\\mbox{Var }X = {\\mathbb E}(X - {\\mathbb E}X)^2$.\n",
    "\n",
    "Algebraically, the following identity holds:\n",
    "$$\n",
    "\\mbox{Var } X = {\\mathbb E}(X - {\\mathbb E}X)^2 = {\\mathbb E}X^2 - 2({\\mathbb E}X)^2 + ({\\mathbb E}X)^2 =\n",
    "{\\mathbb E}X^2 - ({\\mathbb E}X)^2.\n",
    "$$\n",
    "However, this is generally not a good way to calculate $\\mbox{Var} X$ numerically, because of roundoff:\n",
    "it sacrifices precision unnecessarily.\n",
    "\n",
    "The _standard error_ of a random variable $X$ is $\\mbox{SE }X = \\sqrt{\\mbox{Var } X}$.\n",
    "\n",
    "If $\\{X_i\\}_{i=1}^n$ are independent, then $\\mbox{Var} \\sum_{i=1}^n X_i = \\sum_{i=1}^n \\mbox{Var }X_i$.\n",
    "\n",
    "If $X$ and $Y$ have a joint distribution, then $\\mbox{cov} (X,Y) = {\\mathbb E} (X - {\\mathbb E}X)(Y - {\\mathbb E}Y)$.\n",
    "It follows from this definition (and the commutativity of multiplication)\n",
    "that $\\mbox{cov}(X,Y) = \\mbox{cov}(Y,X)$.\n",
    "Also,\n",
    "$$\n",
    "\\mbox{var }(X+Y) = \\mbox{var }X + \\mbox{var }Y + 2\\mbox{cov}(X,Y).\n",
    "$$\n",
    "\n",
    "If $X$ and $Y$ are independent, $\\mbox{cov }(X,Y) = 0$. \n",
    "However, the converse is not necessarily true: $\\mbox{cov}(X,Y) = 0$ does not in general imply that\n",
    "$X$ and $Y$ are independent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Vectors\n",
    "\n",
    "Suppose $\\{X_i\\}_{i=1}^n$ are jointly distributed random variables, and let\n",
    "$$\n",
    "X = \n",
    "\\begin{pmatrix}\n",
    "X_1 \\\\\n",
    "\\vdots \\\\\n",
    "X_n\n",
    "\\end{pmatrix}\n",
    ".\n",
    "$$\n",
    "Then $X$ is a random vector, a $n$ by $1$ vector of real-valued random variables.\n",
    "\n",
    "The expected value of $X$ is\n",
    "$$\n",
    "{\\mathbb E} X =\n",
    "\\begin{pmatrix}\n",
    "{\\mathbb E} X_1 \\\\\n",
    "\\vdots \\\\\n",
    "{\\mathbb E} X_n\n",
    "\\end{pmatrix}\n",
    ".\n",
    "$$\n",
    "\n",
    "The _covariance matrix_ of $X$ is\n",
    "$$\n",
    "\\mbox{cov } X = \n",
    "{\\mathbb E} \n",
    "\\left (\n",
    "\\begin{pmatrix}\n",
    "X_1 - {\\mathbb E} X_1 \\\\\n",
    "\\vdots \\\\\n",
    "X_n - {\\mathbb E} X_n\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "X_1 - {\\mathbb E} X_1 & \\cdots & X_n - {\\mathbb E} X_n\n",
    "\\end{pmatrix}\n",
    "\\right )\n",
    "=\n",
    "{\\mathbb E} \n",
    "\\begin{pmatrix}\n",
    "(X_1 - {\\mathbb E} X_1)^2 & (X_1 - {\\mathbb E} X_1)(X_2 - {\\mathbb E} X_2) & \\cdots & (X_1 - {\\mathbb E} X_1)(X_n - {\\mathbb E} X_n) \\\\\n",
    "(X_1 - {\\mathbb E} X_1)(X_2 - {\\mathbb E} X_2) & (X_2 - {\\mathbb E} X_2)^2 & \\cdots & (X_2 - {\\mathbb E} X_2)(X_n - {\\mathbb E} X_n) \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "(X_1 - {\\mathbb E} X_1)(X_n - {\\mathbb E} X_n) & (X_2 - {\\mathbb E} X_2)(X_n - {\\mathbb E} X_n) & \\cdots & (X_n - {\\mathbb E} X_n)^2\n",
    "\\end{pmatrix}\n",
    ".\n",
    "$$\n",
    "Covariance matrices are always _positive semidefinite_.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Multivariate Normal Distribution\n",
    "\n",
    "The notation $X \\sim {\\mathcal N}(\\mu, \\sigma^2)$ means that $X$ has a normal distribution with mean $\\mu$\n",
    "and variance $\\sigma^2$.\n",
    "This distribution is continuous, with probability density function\n",
    "$$\n",
    "\\frac{1}{\\sqrt{2\\pi} \\sigma} e^{\\frac{-(x-\\mu)^2}{2\\sigma^2}}.\n",
    "$$\n",
    "\n",
    "If $X \\sim {\\mathcal N}(\\mu, \\sigma^2)$, then $\\frac{X-\\mu}{\\sigma} \\sim {\\mathcal N}(0, 1)$,\n",
    "the _standard normal distribution_.\n",
    "The probability density function of the standard normal distribution is\n",
    "$$\n",
    "\\phi(x) = \\frac{1}{\\sqrt{2\\pi}} e^ {\\frac{-x^2}/2}.\n",
    "$$\n",
    "\n",
    "A collection of random variables $\\{ X_1, X_2, \\ldots, X_n\\} = \\{X_j\\}_{j=1}^n$ is _jointly normal_\n",
    "if all linear combinations of those variables have normal distributions.\n",
    "That is, the collection is jointly normal if for all $\\alpha \\in \\Re^n$, $\\sum_{j=1}^n \\alpha_j X_j$\n",
    "has a normal distribution.\n",
    "\n",
    "If $\\{X_j \\}_{j=1}^n$ are independent, normally distributed random variables, they are jointly normal.\n",
    "\n",
    "If for some $\\mu \\in \\Re^n$ and positive-definite matrix $G$, the joint density of $\\{X_j \\}_{j=1}^n$ is\n",
    "$$ \n",
    "\\left ( \\frac{1}{\\sqrt{2 \\pi}}\\right )^n \\frac{1}{\\sqrt{\\left | G \\right |}} \n",
    "\\exp \\left \\{ - \\frac{1}{2} (x - \\mu)'G^{-1}(x-\\mu) \\right \\},\n",
    "$$\n",
    "then $\\{X_j \\}_{j=1}^n$ are jointly normal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Central Limit Theorem\n",
    "\n",
    "Suppose $\\{X_j \\}_{j=1}^\\infty$ are independent and identically distributed (iid), have finite expected value ${\\mathbb E}X_j = \\mu$, and have finite variance $\\mbox{var }X_j = \\sigma^2$.\n",
    "\n",
    "Define the sum $S_n \\equiv \\sum_{j=1}^n X_j$.\n",
    "Then \n",
    "$$\n",
    "{\\mathbb E}S_n = {\\mathbb E} \\sum_{j=1}^n X_j = \\sum_{j=1}^n {\\mathbb E} X_j = \\sum_{j=1}^n \\mu = n\\mu,\n",
    "$$\n",
    "and\n",
    "$$\n",
    "\\mbox{var }S_n = \\mbox{var } \\sum_{j=1}^n X_j = n\\sigma^2.\n",
    "$$\n",
    "(The last step follows from the independence of $\\{X_j\\}$: the variance of the sum is the sum of the variances.)\n",
    "\n",
    "Define $Z_n \\equiv \\frac{S_n - n\\mu}{\\sqrt{n}\\sigma}$.\n",
    "Then for every $a, b \\in \\Re$ with $a \\le b$,\n",
    "$$\n",
    "\\lim_{n \\rightarrow \\infty} {\\mathbb P} \\{ a \\le Z_n \\le b \\} = \\frac{1}{\\sqrt{2\\pi}} \\int_a^b e^{-x^2/2} dx.\n",
    "$$\n",
    "This is a basic form of the Central Limit Theorem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability Inequalities and Identities\n",
    "This follows Lugosi (2006) rather closely; it's also a repeat of the results in the chapter on \n",
    "[Mathematical Preliminaries](mathPrelim.ipynb), which we largely omitted.\n",
    "\n",
    "#### The tail-integral formula for expectation\n",
    "If $X$ is a nonnegative real-valued random variable,\n",
    "$$\n",
    "{\\mathbb E} X = \\int_0^\\infty {\\mathbb P} \\{X \\ge x\\} dx\n",
    "$$\n",
    "\n",
    "#### Jensen's Inequality\n",
    "If $\\phi$ is a convex function from ${\\mathcal X}$ to $\\Re$, then $\\phi({\\mathbb E} X) \\le {\\mathbb E} \\phi(X)$.\n",
    "\n",
    "#### Markov's, Chebychev's, and related inequalities\n",
    "From the tail-integral formula,\n",
    "$$\n",
    "{\\mathbb E} X \\ge \\int_0^t {\\mathbb P} \\{X \\ge x\\} dx \\ge t {\\mathbb P} \\{X \\ge t \\},\n",
    "$$\n",
    "which yields _Markov's Inequality_:\n",
    "$$\n",
    "{\\mathbb P} \\{ X \\ge t \\} \\le \\frac{{\\mathbb E} X}{t}.\n",
    "$$\n",
    "\n",
    "Moreover, for any strictly monotonic function $f$ and nonnegative $X$,\n",
    "we have the _Generalized Markov Inequality_:\n",
    "$$\n",
    "{\\mathbb P} \\{ X \\ge t \\} = {\\mathbb P} \\{ f(X) \\ge f(t) \\} \\le \\frac{{\\mathbb E} f(X)}{f(t)}.\n",
    "$$\n",
    "In particular, for any real-valued $X$ and real $q > 0$,\n",
    "$| X - {\\mathbb E} X |$ is a nonnegative\n",
    "random variable and $f(x) = x^q$ is strictly monotonic, so\n",
    "$$\n",
    "{\\mathbb P} \\{| X - {\\mathbb E} X | \\ge t \\} = {\\mathbb P} \\{ | X - {\\mathbb E} X |^q \\ge t^q \\} \\le\n",
    "\\frac{{\\mathbb E}  | X - {\\mathbb E} X |^q}{t^q}.\n",
    "$$\n",
    "_Chebychev's inequality_ is a special case:\n",
    "$$\n",
    "{\\mathbb P} \\{ | X - {\\mathbb E} X |^2 \\ge t^2 \\} \\le \\frac{{\\mathbb E}  | X - {\\mathbb E} X |^2}{t^2}\n",
    "= \\frac{{\\mbox{Var}} X}{t^2}.\n",
    "$$\n",
    "\n",
    "#### The Chernoff Bound\n",
    "Apply the Generalized Markov Inequality with $f(x) = e^{sx}$ for positive $s$\n",
    "to obtain the _Chernoff Bound_:\n",
    "$$\n",
    "{\\mathbb P} \\{ X \\ge t \\} = {\\mathbb P} \\{ e^{sX} \\ge e^{st} \\} \\le \\frac{{\\mathbb E} e^{sX}}{e^{st}}\n",
    "$$\n",
    "for all $s$.\n",
    "For particular $X$, one can optimize over $s$.\n",
    "\n",
    "\n",
    "#### Hoeffding's Inequality\n",
    "Let $\\{ X_j \\}_{j=1}^n$ be independent, and define\n",
    "$S_n \\equiv \\sum_{j=1}^n X_j$.\n",
    "Applying the Chernoff Bound yields\n",
    "$$\n",
    "{\\mathbb P} \\{ S_n - {\\mathbb E} S_n \\ge t \\} \\le e^{-st} {\\mathbb E} e^{sS_n} =\n",
    "e^{-st} \\prod_{j=1}^n e^{s(X_j - E X_j)}.\n",
    "$$\n",
    "Hoeffding bounds the moment generating function for a bounded random variable\n",
    "$X$:\n",
    "If $a \\le X \\le b$ with probability 1, then\n",
    "$$\n",
    "{\\mathbb E} e^{sX}  \\le e^{s^2 (b-a)^2/8},\n",
    "$$\n",
    "from which follows _Hoeffding's tail bound_.\n",
    "\n",
    "If $\\{X_j\\}_{j=1}^n$ are independent and ${\\mathbb P} \\{a_j \\le X_j \\le b_j\\} = 1$,\n",
    "then\n",
    "$$ \n",
    "{\\mathbb P} \\{ S_n - {\\mathbb E} S_n \\ge t \\} \\le e^{-2t^2/\\sum_{j=1}^n (b_j - a_j)^2}\n",
    "$$\n",
    "and\n",
    "$$ \n",
    "{\\mathbb P} \\{ S_n - {\\mathbb E} S_n \\le -t \\} \\le e^{-2t^2/\\sum_{j=1}^n (b_j - a_j)^2}\n",
    "$$\n",
    "\n",
    "#### Hoeffding's Other Inequality\n",
    "Suppose $f$ is a convex, real function and ${\\mathcal X}$ is a finite set.\n",
    "Let $\\{X_j \\}_{j=1}^n$ be a simple random sample from ${\\mathcal X}$ and\n",
    "let $\\{Y_j \\}_{j=1}^n$ be an iid uniform random sample (with replacement) from ${\\mathcal X}$.\n",
    "Then\n",
    "$$  \n",
    "{\\mathbb E} f \\left ( \\sum_{j=1}^n X_j \\right ) \\le {\\mathbb E} f \\left ( \\sum_{j=1}^n Y_j \\right ).\n",
    "$$\n",
    "\n",
    "#### Bernstein's Inequality\n",
    "Suppose $\\{X_j \\}_{j=1}^n$ are independent with ${\\mathbb E} X_j = 0$ for all $j$,\n",
    "${\\mathbb P} \\{ | X_j | \\le c\\} = 1$,\n",
    "$\\sigma_j^2 = {\\mathbb E} X_j^2$ and $\\sigma = \\frac{1}{n} \\sum_{j=1}^n \\sigma_j^2$.\n",
    "Then for any $\\epsilon > 0$,\n",
    "$$\n",
    "{\\mathbb P} \\{ S_n/n > \\epsilon \\} \\le e^{-n \\epsilon^2 / 2(\\sigma^2 + c\\epsilon/3)}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next chapter: [Inference and Statistical Models](inference.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.1.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
